{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_mood_smartphone.csv\")\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['date'] = df['time'].dt.date\n",
    "df.rename(columns={'Unnamed: 0': 'index'}, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['variable'].isin(['sms', 'call'])]\n",
    "\n",
    "# Aggregate counts by individual and event type\n",
    "counts = df_filtered.groupby(['id', 'variable'])['value'].size().reset_index(name='count')\n",
    "\n",
    "# Determine the IQR for outlier detection\n",
    "Q1 = counts['count'].quantile(0.25)\n",
    "Q3 = counts['count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_threshold_low = Q1 - 1.5 * IQR\n",
    "outliers_threshold_high = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = counts[(counts['count'] < outliers_threshold_low) | (counts['count'] > outliers_threshold_high)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'sms' and 'call' events\n",
    "df_sms_call = df[df['variable'].isin(['sms', 'call'])]\n",
    "\n",
    "# Group by individual ('id'), variable, and date to count daily events\n",
    "daily_counts = df_sms_call.groupby(['id', 'variable', df_sms_call['time'].dt.date]).size().reset_index(name='daily_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for daily counts\n",
    "Q1 = daily_counts['daily_count'].quantile(0.25)\n",
    "Q3 = daily_counts['daily_count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outliers as those beyond 1.5 times the IQR from the quartiles\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outlier rows based on daily counts\n",
    "outliers = daily_counts[(daily_counts['daily_count'] < lower_bound) | (daily_counts['daily_count'] > upper_bound)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows_arousal = df[(df['variable'] == 'circumplex.arousal') & (df['value'].isna())].index\n",
    "nan_rows_valence = df[(df['variable'] == 'circumplex.valence') & (df['value'].isna())].index\n",
    "nan_rows_activity = df[(df['variable'] == 'activity') & (df['value'].isna())].index\n",
    "\n",
    "\n",
    "# Combine the indices of rows with NaN values for arousal and valence\n",
    "nan_rows_combined = nan_rows_arousal.union(nan_rows_valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables/categories to check for negative values, excluding mood, arousal, and valence\n",
    "variables_to_check = [variable for variable in df['variable'].unique() if variable not in ['mood', 'circumplex.arousal', 'circumplex.valence']]\n",
    "\n",
    "# Check for negative values in the remaining variables\n",
    "negative_values_check = {variable: (df[df['variable'] == variable]['value'] < 0).any() for variable in variables_to_check}\n",
    "\n",
    "negative_values_check\n",
    "\n",
    "# Identify rows with negative values in appCat.builtin and appCat.entertainment in the original dataset\n",
    "negative_values_builtin = df[(df['variable'] == 'appCat.builtin') & (df['value'] < 0)].index\n",
    "negative_values_entertainment = df[(df['variable'] == 'appCat.entertainment') & (df['value'] < 0)].index\n",
    "\n",
    "#neg combined \n",
    "neg = negative_values_builtin.union(negative_values_entertainment)\n",
    "# Combine the indices of rows with negative values for appCat.builtin and appCat.entertainment\n",
    "# with previously identified NaN rows for removal\n",
    "remove_combined = nan_rows_combined.union(negative_values_builtin).union(negative_values_entertainment)\n",
    "\n",
    "df_negative = df.loc[neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = df.loc[remove_combined]\n",
    "combined\n",
    "df1 = df.drop(combined.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store outliers\n",
    "outliers_df = pd.DataFrame()\n",
    "\n",
    "variables_for_box_plots_all = [\n",
    "    'screen', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment',\n",
    "    'appCat.finance', 'appCat.game', 'appCat.office', 'appCat.other', 'appCat.social',\n",
    "    'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather'\n",
    "]\n",
    "# Iterate over each variable to remove outliers, saving them first\n",
    "for variable in variables_for_box_plots_all:\n",
    "    # Isolate the current variable's data\n",
    "    var_df = df1[df1['variable'] == variable]\n",
    "    \n",
    "    # Calculate IQR and determine bounds for outliers\n",
    "    Q1 = var_df['value'].quantile(0.25)\n",
    "    Q3 = var_df['value'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identifying outliers\n",
    "    outliers = var_df[(var_df['value'] < lower_bound) | (var_df['value'] > upper_bound)]\n",
    "    \n",
    "    # Append outliers to the outliers_df DataFrame\n",
    "    outliers_df = pd.concat([outliers_df, outliers], ignore_index=True)\n",
    "    \n",
    "    # Identifying indexes of rows that are not outliers to keep in the original dataframe\n",
    "    non_outliers_index = var_df[(var_df['value'] >= lower_bound) & (var_df['value'] <= upper_bound)].index\n",
    "    \n",
    "    # Update df to only include rows that are not outliers for the current variable\n",
    "    df1 = df1[(df1.index.isin(non_outliers_index)) | (df1['variable'] != variable)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_variables = [\"mood\", \"circumplex.arousal\", \"circumplex.valence\", \"activity\"]\n",
    "\n",
    "# Creating a dataset with only the selected variables\n",
    "df_score = df1[df1['variable'].isin(score_variables)]\n",
    "\n",
    "# Creating another dataset with the rest of the variables\n",
    "df_machine = df1[~df1['variable'].isin(score_variables)]\n",
    "\n",
    "df_score['date'] = df_score['time'].dt.date\n",
    "df_machine['date'] = df_machine['time'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_daily = df_score.drop('time', axis = 1)\n",
    "df_machine_daily = df_machine.drop('time', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_scores = df_scores_daily.groupby(['id', 'date', 'variable'])['value'].mean().reset_index()\n",
    "grouped_scores\n",
    "grouped_times = df_machine_daily.groupby(['id', 'date', 'variable'])['value'].sum().reset_index()\n",
    "grouped_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pivot_df = df_scores_daily.pivot_table(index=['id','date'], columns='variable', values='value', aggfunc='mean').reset_index()\n",
    "scores_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pivot_df = df_machine_daily.pivot_table(index=['id','date'], columns='variable', values='value', aggfunc='sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_categories_columns = [\n",
    "    \"appCat.builtin\", \"appCat.communication\", \"appCat.entertainment\",\n",
    "    \"appCat.finance\", \"appCat.game\", \"appCat.office\", \"appCat.other\",\n",
    "    \"appCat.social\", \"appCat.travel\", \"appCat.unknown\", \"appCat.utilities\",\n",
    "    \"appCat.weather\"\n",
    "]\n",
    "\n",
    "# Ensure the DataFrame has these columns; this prevents KeyError if some columns don't exist\n",
    "existing_app_columns = [col for col in app_categories_columns if col in time_pivot_df.columns]\n",
    "\n",
    "# Replace NaN values with 0 for the specified app category columns\n",
    "time_pivot_df[existing_app_columns] = time_pivot_df[existing_app_columns].fillna(0)\n",
    "time_pivot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(scores_pivot_df, time_pivot_df, on=['id','date'], how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW SPLIT INTO TRAIN TEST SETS, THEN ADD ALL INTERPOLATION STUFF TO BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tscv = TimeSeriesSplit(n_splits=2) #ONLY 1 SPLIT = change to 5 for kfoldcross\n",
    "train_df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_interpolate = ['call',  'sms']\n",
    "train_df[columns_to_interpolate] = train_df[columns_to_interpolate].interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "mean_activity_per_id = train_df.groupby('id')['activity'].mean()\n",
    "train_df['activity'] = train_df.apply(\n",
    "    lambda row: mean_activity_per_id[row['id']] if pd.isna(row['activity']) else row['activity'],\n",
    "    axis=1\n",
    ")\n",
    "#test_df[columns_to_interpolate] = test_df[columns_to_interpolate].interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_interpolate = ['call', 'sms', 'activity']\n",
    "train_df[columns_to_interpolate] = train_df.groupby('id')[columns_to_interpolate].transform(lambda group: group.interpolate(method='linear', limit_direction='forward', axis=0))\n",
    "#test_df[columns_to_interpolate] = test_df.groupby('id')[columns_to_interpolate].transform(lambda group: group.interpolate(method='linear', limit_direction='forward', axis=0))\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['mood']\n",
    "\n",
    "\n",
    "train_df_cleaned = train_df.dropna(subset=columns_to_check)\n",
    "#test_df = test_df.dropna(subset=columns_to_check)\n",
    "train_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_columns = [\n",
    "    \"circumplex.valence\"\n",
    "]\n",
    "\n",
    "# Ensure the DataFrame has these columns; this prevents KeyError if some columns don't exist\n",
    "existing_app_columns = [col for col in not_null_columns if col in train_df_cleaned.columns]\n",
    "\n",
    "# Replace NaN values with 0 for the specified app category columns\n",
    "train_df_cleaned[existing_app_columns] = train_df_cleaned[existing_app_columns].fillna(0)\n",
    "#test_df[existing_app_columns] = test_df[existing_app_columns].fillna(0)\n",
    "train_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_categories_columns = [\n",
    "    \"appCat.builtin\", \"appCat.communication\", \"appCat.entertainment\",\n",
    "    \"appCat.finance\", \"appCat.game\", \"appCat.office\", \"appCat.other\",\n",
    "    \"appCat.social\", \"appCat.travel\", \"appCat.unknown\", \"appCat.utilities\",\n",
    "    \"appCat.weather\",\n",
    "]\n",
    "\n",
    "# Ensure the DataFrame has these columns; this prevents KeyError if some columns don't exist\n",
    "existing_app_columns = [col for col in app_categories_columns if col in train_df_cleaned.columns]\n",
    "\n",
    "# Replace NaN values with 0 for the specified app category columns\n",
    "train_df_cleaned[existing_app_columns] = train_df_cleaned[existing_app_columns].fillna(0)\n",
    "#test_df[existing_app_columns] = test_df[existing_app_columns].fillna(0)\n",
    "train_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_sum = [\n",
    "    \"appCat.builtin\", \"appCat.communication\", \"appCat.entertainment\",\n",
    "    \"appCat.finance\", \"appCat.game\", \"appCat.office\", \"appCat.other\",\n",
    "    \"appCat.social\", \"appCat.travel\", \"appCat.unknown\", \"appCat.utilities\",\n",
    "    \"appCat.weather\"\n",
    "]\n",
    "\n",
    "# Use apply to iterate over rows for rows where 'screen' is NaN\n",
    "train_df_cleaned.loc[train_df_cleaned['screen'].isna(), 'screen'] = train_df_cleaned[train_df_cleaned['screen'].isna()].apply(\n",
    "    lambda row: row[columns_to_sum].sum(), axis=1)\n",
    "#test_df.loc[test_df['screen'].isna(), 'screen'] = test_df[test_df['screen'].isna()].apply(\n",
    "    #lambda row: row[columns_to_sum].sum(), axis=1)\n",
    "\n",
    "train_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_columns = [col for col in train_df_cleaned.columns if 'appCat' in col] \n",
    "train_df_cleaned['total_app_usage'] = train_df_cleaned[app_columns].sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming train_df_cleaned is already loaded and prepared\n",
    "# Define weights for each category - adjust these based on your specific requirements\n",
    "weights = {\n",
    "    'appCat.finance': 0.5,      # Weight for finance apps\n",
    "    'appCat.office': 0.5,       # Weight for office apps\n",
    "    'appCat.communication': 0.5,# Weight for communication apps\n",
    "    'appCat.social': 0.5,       # Weight for social apps\n",
    "    'appCat.entertainment': 0.5 # Weight for entertainment apps\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'dff' is your DataFrame\n",
    "# Identify columns related to app usage\n",
    "app_columns = [col for col in train_df_cleaned.columns if 'appCat.' in col]\n",
    "\n",
    "# Sum these columns to get a total time spent on app components\n",
    "train_df_cleaned['total_app_time'] = train_df_cleaned[app_columns].sum(axis=1)\n",
    "\n",
    "# Now, 'dff' includes a new column 'total_app_time' that has the total time spent on apps for each entry\n",
    "\n",
    "\n",
    "# Calculate weighted sums\n",
    "train_df_cleaned['productivity_attribute'] = (\n",
    "    train_df_cleaned['appCat.finance'] * weights['appCat.finance'] +\n",
    "    train_df_cleaned['appCat.office'] * weights['appCat.office']\n",
    ")\n",
    "\n",
    "train_df_cleaned['social_app'] = (\n",
    "    train_df_cleaned['appCat.communication'] * weights['appCat.communication'] +\n",
    "    train_df_cleaned['appCat.social'] * weights['appCat.social']\n",
    ")\n",
    "\n",
    "# Calculate weighted ratio for productivity to social apps\n",
    "# Adding 1 to avoid division by zero in case totals are zero\n",
    "train_df_cleaned['productivity_to_social_app_ratio'] = (\n",
    "    (train_df_cleaned['appCat.finance'] * weights['appCat.finance'] +\n",
    "    train_df_cleaned['appCat.office'] * weights['appCat.office'] + 1) /\n",
    "    (train_df_cleaned['appCat.entertainment'] * weights['appCat.entertainment'] +\n",
    "    train_df_cleaned['appCat.social'] * weights['appCat.social'] + 1)\n",
    ")\n",
    "\n",
    "# Assuming sms and call are equally important for 'social_phone'\n",
    "train_df_cleaned['social_phone'] = (\n",
    "    train_df_cleaned['sms'] * 0.5 +  # Assuming equal weight for sms\n",
    "    train_df_cleaned['call'] * 0.5   # Assuming equal weight for calls\n",
    ")\n",
    "\n",
    "# Display the modified DataFrame\n",
    "train_df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agg Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'grade' is your continuous variable\n",
    "train_df_cleaned['mood_quantiles'] = pd.qcut(train_df_cleaned['mood'], q=3, labels=['Q1', 'Q2', 'Q3'])\n",
    "#test_df['mood_quantiles'] = pd.qcut(test_df['mood'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "train_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert string labels to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df_cleaned['mood_quantiles'] = label_encoder.fit_transform(train_df_cleaned['mood_quantiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_moving_averages(df):\n",
    "    # Set 'date' as the index for rolling calculations\n",
    "    df = df.set_index('date')\n",
    "    \n",
    "    # Calculate moving averages with shifting to avoid data leakage\n",
    "    ma_3days = df.rolling(window=3).mean().shift(1)\n",
    "    #ma_5days = df.rolling(window=5).mean().shift(1)\n",
    "    ma_7days = df.rolling(window=7).mean().shift(1)\n",
    "    \n",
    "    # Add suffixes to identify the columns for each moving average\n",
    "    ma_3days.columns = [f\"{col}_3day_avg\" for col in ma_3days.columns]\n",
    "    #ma_5days.columns = [f\"{col}_5day_avg\" for col in ma_5days.columns]\n",
    "    ma_7days.columns = [f\"{col}_7day_avg\" for col in ma_7days.columns]\n",
    "    \n",
    "    # Concatenate the original data with the moving averages\n",
    "    result = pd.concat([df, ma_3days, ma_7days], axis=1)\n",
    "    \n",
    "    # Handle NaN values: forward fill first, then backward fill\n",
    "    result.ffill(inplace=True)\n",
    "    result.bfill(inplace=True)\n",
    "    \n",
    "    return result.reset_index()  # Reset the index to bring 'date' back to a column\n",
    "\n",
    "\n",
    "train_df_cleaned = train_df_cleaned.drop('mood', axis = 1)\n",
    "# Assuming 'train_df_cleaned' is your DataFrame and 'id' is the group identifier\n",
    "corrected_grouped_data = train_df_cleaned.groupby('id').apply(calculate_moving_averages).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'date' column has been converted to datetime format already\n",
    "corrected_grouped_data['date'] = pd.to_datetime(corrected_grouped_data['date'], errors='coerce')\n",
    "\n",
    "# Boolean encoding for 'is_weekend' (0 for weekdays, 1 for weekends)\n",
    "corrected_grouped_data['is_weekend'] = corrected_grouped_data['date'].dt.dayofweek >= 5\n",
    "corrected_grouped_data['is_weekend'] = corrected_grouped_data['is_weekend'].astype(int)  # Convert boolean to integer (0 or 1)\n",
    "\n",
    "# Extract the day of the week (0=Monday, 6=Sunday)\n",
    "corrected_grouped_data['day_of_week'] = corrected_grouped_data['date'].dt.dayofweek\n",
    "\n",
    "# Extract the month from the date (1=January, 12=December)\n",
    "corrected_grouped_data['month'] = corrected_grouped_data['date'].dt.month\n",
    "\n",
    "# One-hot encode 'day_of_week' and 'month'\n",
    "day_of_week_dummies = pd.get_dummies(corrected_grouped_data['day_of_week'], prefix='day')\n",
    "month_dummies = pd.get_dummies(corrected_grouped_data['month'], prefix='month')\n",
    "\n",
    "# Concatenate the original data frame with the new one-hot encoded columns\n",
    "final_data = pd.concat([corrected_grouped_data, day_of_week_dummies, month_dummies], axis=1)\n",
    "\n",
    "# Now 'data' includes the original columns plus the new one-hot encoded columns for day of the week and month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('data_class.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
